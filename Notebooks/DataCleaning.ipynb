{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07feb961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0335a9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://data.transportation.gov/resource/8ect-6jqj.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83494098",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_ngsim_api(location=None, direction=None, lane=None,\n",
    "                    t_start_ms=None, t_end_ms=None,\n",
    "                    limit=50000, max_pages=20, app_token=None):\n",
    "    headers = {}\n",
    "    if app_token:\n",
    "        headers[\"X-App-Token\"] = app_token\n",
    "\n",
    "    where_clauses = []\n",
    "    if location:  where_clauses.append(f\"location='{location}'\")\n",
    "    if direction is not None: where_clauses.append(f\"direction='{direction}'\")\n",
    "    if lane is not None:      where_clauses.append(f\"lane_id='{lane}'\")\n",
    "    if t_start_ms is not None: where_clauses.append(f\"global_time >= '{t_start_ms}'\")\n",
    "    if t_end_ms is not None:   where_clauses.append(f\"global_time <= '{t_end_ms}'\")\n",
    "    where = \" AND \".join(where_clauses) if where_clauses else None\n",
    "\n",
    "    params = {\n",
    "        \"$select\": \"vehicle_id,frame_id,global_time,global_x,global_y,local_x,local_y,v_vel,lane_id,direction,location\",\n",
    "        \"$order\": \"global_time ASC\",\n",
    "        \"$limit\": limit\n",
    "    }\n",
    "    if where:\n",
    "        params[\"$where\"] = where\n",
    "\n",
    "    frames = []\n",
    "    offset = 0\n",
    "    for _ in range(max_pages):\n",
    "        params[\"$offset\"] = offset\n",
    "        r = requests.get(BASE_URL, params=params, headers=headers, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        chunk = r.json()\n",
    "        if not chunk:\n",
    "            break\n",
    "        frames.append(pd.DataFrame(chunk))\n",
    "        offset += limit\n",
    "    if not frames:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    # Types & units\n",
    "    to_float = [\"global_time\",\"global_x\",\"global_y\",\"local_x\",\"local_y\",\"v_vel\"]\n",
    "    to_int   = [\"frame_id\",\"lane_id\",\"direction\"]\n",
    "    for c in to_float:\n",
    "        if c in df.columns: df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    for c in to_int:\n",
    "        if c in df.columns: df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # Time to seconds\n",
    "    if \"global_time\" in df and df[\"global_time\"].max() > 1e6:\n",
    "        df[\"t\"] = df[\"global_time\"] / 1000.0\n",
    "    else:\n",
    "        # Fallback from frame_id (10Hz)\n",
    "        df[\"t\"] = df[\"frame_id\"] / 10.0\n",
    "\n",
    "    # Rename for consistency; use Global_X as longitudinal x\n",
    "    df = df.rename(columns={\n",
    "        \"global_x\":\"x\", \"v_vel\":\"speed\", \"lane_id\":\"lane\",\n",
    "        \"direction\":\"dir\", \"location\":\"loc\"\n",
    "    })\n",
    "\n",
    "    # mph -> m/s if needed (this dataset looks like mph)\n",
    "    if df[\"speed\"].dropna().median() > 70:\n",
    "        df[\"speed\"] = df[\"speed\"] * 0.44704\n",
    "\n",
    "    return df[[\"vehicle_id\",\"t\",\"x\",\"speed\",\"lane\",\"dir\",\"loc\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "535f884a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  vehicle_id          t            x  speed  lane  dir        loc\n",
      "0         69  1163050.0  2230523.390   1.50     1    2  peachtree\n",
      "1         70  1163050.0  2230522.584   1.60     1    2  peachtree\n",
      "2         17  1163050.0  2230521.592   0.00     1    2  peachtree\n",
      "3         13  1163050.0  2230568.402   3.63     1    2  peachtree\n",
      "4         67  1163050.0  2230526.199   0.00     1    2  peachtree (117014, 7)\n"
     ]
    }
   ],
   "source": [
    "# Choose a 10-minute window by epoch ms (example values)\n",
    "t0_ms = 1163050000  # replace with your desired start\n",
    "t1_ms = 1163050000 + 10*60*1000\n",
    "\n",
    "df = fetch_ngsim_api(location=\"peachtree\", direction=2, lane=1,\n",
    "                     t_start_ms=t0_ms, t_end_ms=t1_ms, limit=50000)\n",
    "print(df.head(), df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d22e5fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecd0b7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_to_grid_df(df, minutes=10, dx=10.0, dt=1.0):\n",
    "    # Ensure we have at least some rows\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No data after API filtering\")\n",
    "\n",
    "    # Time slice to exactly minutes duration from min t\n",
    "    t0 = df[\"t\"].min()\n",
    "    t1 = t0 + 60*minutes\n",
    "    df = df[(df[\"t\"]>=t0) & (df[\"t\"]<=t1)].copy()\n",
    "\n",
    "    # Spatial slice (trim outliers)\n",
    "    x_min = df[\"x\"].quantile(0.01)\n",
    "    x_max = df[\"x\"].quantile(0.99)\n",
    "    df = df[df[\"x\"].between(x_min, x_max)]\n",
    "\n",
    "    # Define bins and centers\n",
    "    x_bins = np.arange(x_min, x_max+dx, dx)\n",
    "    t_bins = np.arange(t0, t1+dt, dt)\n",
    "    x_c = (x_bins[:-1] + x_bins[1:]) / 2\n",
    "    t_c = (t_bins[:-1] + t_bins[1:]) / 2\n",
    "\n",
    "    Ix, Jt = len(x_c), len(t_c)\n",
    "    rho = np.full((Ix, Jt), np.nan)\n",
    "    u   = np.full((Ix, Jt), np.nan)\n",
    "\n",
    "    for j,(ta,tb) in enumerate(zip(t_bins[:-1], t_bins[1:])):\n",
    "        df_t = df[(df[\"t\"]>=ta)&(df[\"t\"]<tb)]\n",
    "        if df_t.empty: continue\n",
    "        idx = np.digitize(df_t[\"x\"].values, x_bins) - 1\n",
    "        m = (idx>=0) & (idx<Ix)\n",
    "        idx = idx[m]; spd = df_t[\"speed\"].values[m]\n",
    "        counts = np.bincount(idx, minlength=Ix)\n",
    "        sums   = np.bincount(idx, weights=spd, minlength=Ix)\n",
    "        with np.errstate(invalid=\"ignore\"):\n",
    "            u[:,j]   = np.where(counts>0, sums/counts, np.nan)\n",
    "            rho[:,j] = np.where(counts>0, counts/dx, np.nan)  # veh/m\n",
    "\n",
    "    mask = ~np.isnan(rho)\n",
    "    rho_f = np.where(mask, rho, 0.0)\n",
    "    u_f   = np.where(mask, u,   0.0)\n",
    "\n",
    "    rho_max = np.nanpercentile(rho, 99)\n",
    "    v_free  = np.nanpercentile(u, 95)\n",
    "    rho_nd  = np.clip(rho_f/rho_max, 0, 1)\n",
    "    u_nd    = np.clip(u_f/v_free,    0, 1)\n",
    "\n",
    "    x_nd = (x_c - x_c.min())/(x_c.max()-x_c.min())\n",
    "    t_nd = (t_c - t_c.min())/(t_c.max()-t_c.min())\n",
    "    Xg, Tg = np.meshgrid(x_nd, t_nd, indexing=\"ij\")\n",
    "\n",
    "    tensors = {\n",
    "        \"x\": torch.tensor(Xg.reshape(-1,1), dtype=torch.float32),\n",
    "        \"t\": torch.tensor(Tg.reshape(-1,1), dtype=torch.float32),\n",
    "        \"rho\": torch.tensor(rho_nd.reshape(-1,1), dtype=torch.float32),\n",
    "        \"u\": torch.tensor(u_nd.reshape(-1,1), dtype=torch.float32),\n",
    "        \"mask\": torch.tensor(mask.reshape(-1,1), dtype=torch.float32),\n",
    "        \"rho_max\": float(rho_max),\n",
    "        \"v_free\": float(v_free),\n",
    "        \"x_centers\": x_c, \"t_centers\": t_c\n",
    "    }\n",
    "    return tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4357ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = aggregate_to_grid_df(df, minutes=10, dx=10.0, dt=1.0)\n",
    "x, t, rho, u, mask = tensors[\"x\"], tensors[\"t\"], tensors[\"rho\"], tensors[\"u\"], tensors[\"mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f583e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def save_grid_as_csv(tensors, base_path=\"ngsim_grid\"):\n",
    "    x = tensors[\"x\"].numpy().ravel()\n",
    "    t = tensors[\"t\"].numpy().ravel()\n",
    "    rho = tensors[\"rho\"].numpy().ravel()\n",
    "    u = tensors[\"u\"].numpy().ravel()\n",
    "    mask = tensors[\"mask\"].numpy().ravel().astype(np.int32)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"x_nd\": x,\n",
    "        \"t_nd\": t,\n",
    "        \"rho_nd\": rho,\n",
    "        \"u_nd\": u,\n",
    "        \"mask\": mask\n",
    "    })\n",
    "    df.to_csv(f\"{base_path}.csv\", index=False)\n",
    "\n",
    "    pd.DataFrame({\"x_centers_m\": tensors[\"x_centers\"]}).to_csv(f\"{base_path}_x_centers.csv\", index=False)\n",
    "    pd.DataFrame({\"t_centers_s\": tensors[\"t_centers\"]}).to_csv(f\"{base_path}_t_centers.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ebf14c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_grid_as_csv(tensors, base_path=\"peachtree_dir2_lane1_10m_1s_10min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c812f590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
